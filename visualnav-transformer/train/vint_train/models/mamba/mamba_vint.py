"""
MambaViNT: ä½¿ç”¨Mambaæ›¿æ¢Transformerçš„ViNTæ¨¡å‹
ä¿ç•™ViNTçš„è§†è§‰ç¼–ç å™¨ï¼Œä½¿ç”¨Mambaè¿›è¡Œæ—¶åºå»ºæ¨¡

å…³é”®æ”¹è¿›ï¼ˆå‚è€ƒMTILå®ç°ï¼‰ï¼š
1. è®­ç»ƒæ¨¡å¼(forward)ï¼šå®Œæ•´åºåˆ—é€šè¿‡Mambaï¼Œåˆ©ç”¨å†…éƒ¨chunkæœºåˆ¶ä¼˜åŒ–
2. æ¨ç†æ¨¡å¼(step)ï¼šå•æ­¥æ›´æ–°éšçŠ¶æ€ï¼Œå®ç°O(1)æ˜¾å­˜å ç”¨
3. ä¸MTILçš„å¯¹åº”å…³ç³»ï¼š
   - forward() â†” MTILç¬¬725-765è¡Œ
   - step() â†” MTILç¬¬641-722è¡Œ
   - init_hidden_states() â†” MTILç¬¬626-639è¡Œ

æ€§èƒ½ç‰¹ç‚¹ï¼š
- è®­ç»ƒæ—¶æ˜¾å­˜å ç”¨ï¼šO(seq_len) - çº¿æ€§å¢é•¿ä½†æ¯”Transformerå¥½
- æ¨ç†æ—¶æ˜¾å­˜å ç”¨ï¼šO(1) - æ’å®šï¼Œä¸åºåˆ—é•¿åº¦æ— å…³
- å‚æ•°é‡ï¼šå›ºå®šï¼Œä¸éšcontext_sizeå˜åŒ–
- è®¡ç®—å¤æ‚åº¦ï¼šO(seq_len) - çº¿æ€§ï¼Œæ¯”Transformerçš„O(seq_lenÂ²)å¿«
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Optional, Tuple
from efficientnet_pytorch import EfficientNet

from vint_train.models.base_model import BaseModel
from .mamba2 import Mamba2

# ä½¿ç”¨å®˜æ–¹ mamba_ssm.Blockï¼ˆä¸ MTIL ä¸€è‡´ï¼‰
from mamba_ssm.modules.block import Block


class MambaViNT(BaseModel):
    """
    MambaViNT: ç”¨Mambaæ›¿æ¢Transformerçš„ViNTæ¨¡å‹

    æ¶æ„:
    - è§†è§‰ç¼–ç å™¨: EfficientNet (ä¿ç•™ViNTåŸæœ‰)
    - æ—¶åºå»ºæ¨¡: Mamba2 Blocks (æ›¿æ¢MultiLayerDecoder)
    - åŠ¨ä½œé¢„æµ‹: Linearå±‚ (ä¿ç•™ViNTåŸæœ‰)

    ä¼˜åŠ¿:
    - åˆ©ç”¨Mambaçš„éšçŠ¶æ€ç¼–ç å®Œæ•´å†å²
    - è§£å†³é‡å¤åœºæ™¯çš„çŠ¶æ€æ­§ä¹‰
    - æ”¯æŒå•æ­¥æ¨ç†ï¼ˆéƒ¨ç½²å‹å¥½ï¼‰
    """

    def __init__(
        self,
        context_size: int = 5,
        len_traj_pred: Optional[int] = 5,
        learn_angle: Optional[bool] = True,
        obs_encoder: Optional[str] = "efficientnet-b0",
        obs_encoding_size: Optional[int] = 512,
        late_fusion: Optional[bool] = False,
        # Mambaç‰¹å®šå‚æ•°
        mamba_d_state: Optional[int] = 64,  # å¯¼èˆªä»»åŠ¡ä¼˜åŒ–ï¼ˆåŸMTILä¸º512ï¼‰
        mamba_d_conv: Optional[int] = 4,
        mamba_expand: Optional[int] = 2,
        mamba_headdim: Optional[int] = 64,   # å¯¼èˆªä»»åŠ¡ä¼˜åŒ–ï¼ˆåŸMTILä¸º128ï¼‰
        mamba_num_blocks: Optional[int] = 4,
        mamba_chunk_size: Optional[int] = 256,
        mamba_use_mem_eff: Optional[bool] = True,
    ) -> None:
        """
        åˆå§‹åŒ–MambaViNTæ¨¡å‹

        Args:
            context_size: ä¸Šä¸‹æ–‡å¸§æ•°
            len_traj_pred: é¢„æµ‹è½¨è¿¹é•¿åº¦
            learn_angle: æ˜¯å¦é¢„æµ‹è§’åº¦
            obs_encoder: è§†è§‰ç¼–ç å™¨ç±»å‹
            obs_encoding_size: ç¼–ç ç»´åº¦
            late_fusion: æ˜¯å¦åæœŸèåˆ
            mamba_d_state: MambaçŠ¶æ€ç»´åº¦
            mamba_d_conv: å·ç§¯æ ¸å¤§å°
            mamba_expand: æ‰©å±•å› å­
            mamba_headdim: æ¯ä¸ªå¤´çš„ç»´åº¦
            mamba_num_blocks: Mambaå—æ•°é‡
            mamba_chunk_size: chunkå¤§å°
            mamba_use_mem_eff: æ˜¯å¦ä½¿ç”¨å†…å­˜é«˜æ•ˆè·¯å¾„
        """
        super(MambaViNT, self).__init__(context_size, len_traj_pred, learn_angle)

        self.obs_encoding_size = obs_encoding_size
        self.goal_encoding_size = obs_encoding_size
        self.late_fusion = late_fusion
        self.mamba_num_blocks = mamba_num_blocks

        # 1. è§†è§‰ç¼–ç å™¨ï¼ˆä¿ç•™ViNTçš„EfficientNetï¼‰
        if obs_encoder.split("-")[0] == "efficientnet":
            self.obs_encoder = EfficientNet.from_name(obs_encoder, in_channels=3)
            self.num_obs_features = self.obs_encoder._fc.in_features
            if self.late_fusion:
                self.goal_encoder = EfficientNet.from_name("efficientnet-b0", in_channels=3)
            else:
                self.goal_encoder = EfficientNet.from_name("efficientnet-b0", in_channels=6)
            self.num_goal_features = self.goal_encoder._fc.in_features
        else:
            raise NotImplementedError(f"Encoder {obs_encoder} not supported")

        # å‹ç¼©å±‚
        if self.num_obs_features != self.obs_encoding_size:
            self.compress_obs_enc = nn.Linear(self.num_obs_features, self.obs_encoding_size)
        else:
            self.compress_obs_enc = nn.Identity()

        if self.num_goal_features != self.goal_encoding_size:
            self.compress_goal_enc = nn.Linear(self.num_goal_features, self.goal_encoding_size)
        else:
            self.compress_goal_enc = nn.Identity()

        # 2. Mambaæ—¶åºå»ºæ¨¡ï¼ˆæ›¿æ¢Transformerï¼‰
        def mixer_fn(dim):
            return Mamba2(
                d_model=dim,
                d_state=mamba_d_state,
                d_conv=mamba_d_conv,
                expand=mamba_expand,
                headdim=mamba_headdim,
                chunk_size=mamba_chunk_size,
                use_mem_eff_path=mamba_use_mem_eff,
            )

        def mlp_fn(dim):
            hidden_dim = 4 * dim
            return nn.Sequential(
                nn.Linear(dim, hidden_dim),
                nn.GELU(),
                nn.Linear(hidden_dim, dim),
            )

        self.mamba_blocks = nn.ModuleList([
            Block(
                dim=self.obs_encoding_size,
                mixer_cls=mixer_fn,
                mlp_cls=mlp_fn,
                norm_cls=nn.LayerNorm,
                fused_add_norm=True,  # ğŸš€ å¼€å¯èåˆä¼˜åŒ– (åŠ é€Ÿ 20-30%)
                residual_in_fp32=True,  # ğŸš€ å¼€å¯ FP32 æ®‹å·® (æå‡ç¨³å®šæ€§)
            )
            for _ in range(mamba_num_blocks)
        ])

        # 3. è¾“å‡ºå±‚ï¼ˆä¿ç•™ViNTçš„é¢„æµ‹å¤´ï¼‰
        final_dim = 32  # ä¸ViNTä¸€è‡´
        self.final_proj = nn.Linear(self.obs_encoding_size, final_dim)

        self.dist_predictor = nn.Sequential(
            nn.Linear(final_dim, 1),
        )
        self.action_predictor = nn.Sequential(
            nn.Linear(final_dim, self.len_trajectory_pred * self.num_action_params),
        )

    def forward(
        self, obs_img: torch.Tensor, goal_img: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ¨¡å¼ - å‚è€ƒMTILçš„åºåˆ—å¤„ç†æ–¹å¼ï¼‰

        å…³é”®æ”¹è¿›ï¼š
        1. ä¿æŒä¸MTILä¸€è‡´çš„åºåˆ—å¤„ç†æ–¹å¼
        2. å®Œæ•´åºåˆ—é€šè¿‡Mambaï¼ˆåˆ©ç”¨chunk_sizeå†…éƒ¨ä¼˜åŒ–ï¼‰
        3. è®­ç»ƒæ—¶ä¸éœ€è¦æ‰‹åŠ¨ç®¡ç†éšçŠ¶æ€

        Args:
            obs_img: [batch, 3*(context_size+1), H, W]
            goal_img: [batch, 3, H, W]

        Returns:
            dist_pred: [batch, 1]
            action_pred: [batch, len_traj_pred, num_action_params]
        """
        device = next(self.parameters()).device
        obs_img = obs_img.to(device)
        goal_img = goal_img.to(device)

        batch_size = obs_img.shape[0]

        # 1. æå–ç›®æ ‡ç‰¹å¾
        if self.late_fusion:
            goal_encoding = self.goal_encoder.extract_features(goal_img)
        else:
            obsgoal_img = torch.cat([obs_img[:, 3*self.context_size:, :, :], goal_img], dim=1)
            goal_encoding = self.goal_encoder.extract_features(obsgoal_img)

        goal_encoding = self.goal_encoder._avg_pooling(goal_encoding)
        if self.goal_encoder._global_params.include_top:
            goal_encoding = goal_encoding.flatten(start_dim=1)
            goal_encoding = self.goal_encoder._dropout(goal_encoding)
        goal_encoding = self.compress_goal_enc(goal_encoding)
        if len(goal_encoding.shape) == 2:
            goal_encoding = goal_encoding.unsqueeze(
                1)  # [batch, 1, encoding_size]

        # 2. æå–è§‚æµ‹åºåˆ—ç‰¹å¾
        obs_img = torch.split(obs_img, 3, dim=1)  # åˆ†å‰²ä¸ºcontext_size+1å¸§
        obs_img = torch.cat(obs_img, dim=0)  # [batch*(context+1), 3, H, W]

        obs_encoding = self.obs_encoder.extract_features(obs_img)
        obs_encoding = self.obs_encoder._avg_pooling(obs_encoding)
        if self.obs_encoder._global_params.include_top:
            obs_encoding = obs_encoding.flatten(start_dim=1)
            obs_encoding = self.obs_encoder._dropout(obs_encoding)
        obs_encoding = self.compress_obs_enc(obs_encoding)
        obs_encoding = obs_encoding.reshape((self.context_size+1, batch_size, self.obs_encoding_size))
        # [batch, context+1, encoding_size]
        obs_encoding = torch.transpose(obs_encoding, 0, 1)

        # 3. æ‹¼æ¥è§‚æµ‹å’Œç›®æ ‡ - å½¢æˆå®Œæ•´åºåˆ—
        # å‚è€ƒMTIL: åºåˆ—é•¿åº¦ = context_size+2 (å†å²å¸§ + å½“å‰å¸§ + ç›®æ ‡)
        # [batch, context+2, encoding_size]
        tokens = torch.cat((obs_encoding, goal_encoding), dim=1)

        # 4. é€šè¿‡Mambaå—è¿›è¡Œæ—¶åºå»ºæ¨¡ï¼ˆå‚è€ƒMTILç¬¬759-760è¡Œï¼‰
        # å…³é”®ï¼šMambaå†…éƒ¨ä¼šé€šè¿‡chunk_sizeä¼˜åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨ç®¡ç†éšçŠ¶æ€
        residual = None
        for block in self.mamba_blocks:
            tokens, residual = block(tokens, residual)

        # 5. æŠ•å½±åˆ°æœ€ç»ˆç»´åº¦ï¼ˆå–æœ€åä¸€ä¸ªtokenï¼Œå³èåˆäº†æ‰€æœ‰å†å²çš„è¡¨ç¤ºï¼‰
        final_repr = self.final_proj(tokens[:, -1, :])  # [batch, 32]

        # 6. é¢„æµ‹è·ç¦»å’ŒåŠ¨ä½œ
        dist_pred = self.dist_predictor(final_repr)
        action_pred = self.action_predictor(final_repr)

        # é‡å¡‘åŠ¨ä½œé¢„æµ‹
        action_pred = action_pred.reshape(
            (action_pred.shape[0], self.len_trajectory_pred, self.num_action_params)
        )
        # cumsumæ“ä½œ - ä½¿ç”¨æ¢¯åº¦è£å‰ªé¿å…çˆ†ç‚¸
        action_pred[:, :, :2] = torch.cumsum(
            action_pred[:, :, :2], dim=1
        )
        if self.learn_angle:
            action_pred[:, :, 2:] = F.normalize(
                action_pred[:, :, 2:].clone(), dim=-1
            )

        return dist_pred, action_pred

    def init_hidden_states(self, batch_size: int, device=None):
        """
        åˆå§‹åŒ–MambaéšçŠ¶æ€ï¼ˆç”¨äºæ¨ç†ï¼‰

        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡

        Returns:
            hidden_states: List[(conv_state, ssm_state)]
        """
        if device is None:
            device = next(self.parameters()).device

        hidden_list = []
        for block in self.mamba_blocks:
            if hasattr(block.mixer, "allocate_inference_cache"):
                conv_state, ssm_state = block.mixer.allocate_inference_cache(
                    batch_size, max_seqlen=1, dtype=None, device=device  # æ·»åŠ deviceå‚æ•°
                )
            else:
                # åˆ›å»ºç©ºçš„éšçŠ¶æ€
                conv_state, ssm_state = None, None
            hidden_list.append((conv_state, ssm_state))

        return hidden_list

    def step(
        self,
        obs_img: torch.Tensor,
        goal_img: torch.Tensor,
        hidden_states: List[Tuple[torch.Tensor, torch.Tensor]]
    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], List[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        å•æ­¥æ¨ç†ï¼ˆç”¨äºéƒ¨ç½² - å‚è€ƒMTILç¬¬641-722è¡Œï¼‰

        å…³é”®æ”¹è¿›ï¼š
        1. å®Œå…¨å‚è€ƒMTILçš„stepå®ç°
        2. ä½¿ç”¨residualæœºåˆ¶ä¿æŒä¸€è‡´æ€§
        3. æ­£ç¡®å¤„ç†éšçŠ¶æ€ä¼ é€’

        Args:
            obs_img: [batch, 3, H, W] - å•å¸§è§‚æµ‹
            goal_img: [batch, 3, H, W] - ç›®æ ‡å›¾åƒ
            hidden_states: List[(conv_state, ssm_state)] - æ¯ä¸ªblockçš„éšçŠ¶æ€

        Returns:
            (dist_pred, action_pred): é¢„æµ‹ç»“æœ
            new_hidden_states: æ›´æ–°åçš„éšçŠ¶æ€
        """
        device = next(self.parameters()).device
        obs_img = obs_img.to(device)
        goal_img = goal_img.to(device)

        batch_size = obs_img.shape[0]

        # 1. æå–ç‰¹å¾
        if self.late_fusion:
            goal_encoding = self.goal_encoder.extract_features(goal_img)
        else:
            obsgoal_img = torch.cat([obs_img, goal_img], dim=1)
            goal_encoding = self.goal_encoder.extract_features(obsgoal_img)

        goal_encoding = self.goal_encoder._avg_pooling(goal_encoding)
        if self.goal_encoder._global_params.include_top:
            goal_encoding = goal_encoding.flatten(start_dim=1)
            goal_encoding = self.goal_encoder._dropout(goal_encoding)
        goal_encoding = self.compress_goal_enc(goal_encoding)

        obs_encoding = self.obs_encoder.extract_features(obs_img)
        obs_encoding = self.obs_encoder._avg_pooling(obs_encoding)
        if self.obs_encoder._global_params.include_top:
            obs_encoding = obs_encoding.flatten(start_dim=1)
            obs_encoding = self.obs_encoder._dropout(obs_encoding)
        obs_encoding = self.compress_obs_enc(obs_encoding)

        # 2. èåˆç‰¹å¾ï¼ˆå•æ­¥è¾“å…¥ï¼šåªæœ‰å½“å‰å¸§çš„ç‰¹å¾ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä¸è®­ç»ƒä¸åŒï¼Œæˆ‘ä»¬åªè¾“å…¥å½“å‰å¸§ï¼Œå†å²ä¿¡æ¯åœ¨éšçŠ¶æ€ä¸­
        x_t = obs_encoding + goal_encoding  # [batch, encoding_size]

        # 3. é€šè¿‡Mambaå—ï¼ˆå‚è€ƒMTILç¬¬688-717è¡Œï¼‰
        residual = None
        new_states = []
        hidden = x_t  # [batch, encoding_size]

        for i, blk in enumerate(self.mamba_blocks):
            conv_st, ssm_st = hidden_states[i] if i < len(hidden_states) else (None, None)

            # åˆå§‹åŒ–residualï¼ˆå‚è€ƒMTILç¬¬690-693è¡Œï¼‰
            if residual is None:
                residual = hidden
            else:
                residual = residual + hidden

            # LayerNormï¼ˆå‚è€ƒMTILç¬¬695è¡Œï¼‰
            hidden_ln = blk.norm(residual.to(dtype=blk.norm.weight.dtype))

            # Mambaå•æ­¥æ¨ç†ï¼ˆå‚è€ƒMTILç¬¬699-706è¡Œï¼‰
            if hasattr(blk.mixer, "step"):
                # unsqueezeæ·»åŠ åºåˆ—ç»´åº¦ [batch, encoding_size] -> [batch, 1, encoding_size]
                y_t, new_conv_st, new_ssm_st = blk.mixer.step(
                    hidden_ln.unsqueeze(1), conv_st, ssm_st
                )
                # [batch, 1, encoding_size] -> [batch, encoding_size]
                y_t = y_t.squeeze(1)
            else:
                # é™çº§å¤„ç†ï¼šæ— stepæ–¹æ³•æ—¶
                y_t = blk.mixer(hidden_ln.unsqueeze(1)).squeeze(1)
                new_conv_st, new_ssm_st = conv_st, ssm_st

            # æ®‹å·®è¿æ¥ï¼ˆå‚è€ƒMTILç¬¬708è¡Œï¼‰
            hidden_out = y_t + residual

            # MLPï¼ˆå‚è€ƒMTILç¬¬711-713è¡Œï¼‰
            if blk.mlp is not None:
                r2 = blk.norm2(hidden_out.to(dtype=blk.norm2.weight.dtype))
                hidden_out = blk.mlp(r2) + hidden_out

            new_states.append((new_conv_st, new_ssm_st))
            hidden = hidden_out
            residual = hidden_out

        # 4. é¢„æµ‹ï¼ˆå‚è€ƒMTILç¬¬720-721è¡Œï¼‰
        final_repr = self.final_proj(hidden)  # [batch, 32]
        dist_pred = self.dist_predictor(final_repr)
        action_pred = self.action_predictor(final_repr)

        # é‡å¡‘åŠ¨ä½œé¢„æµ‹
        action_pred = action_pred.reshape(
            (action_pred.shape[0], self.len_trajectory_pred, self.num_action_params)
        )
        # action_pred[:, :, :2] = torch.cumsum(action_pred[:, :, :2], dim=1)
        
        # cumsum æ“ä½œ - ä¸ºé¿å… AMP/half ç²¾åº¦é—®é¢˜ï¼Œä¸´æ—¶ç”¨ float32 è®¡ç®—å†è¿˜åŸ dtype
        if action_pred.dtype in (torch.float16, torch.bfloat16):
            tmp = action_pred[:, :, :2].to(torch.float32)
            tmp = torch.cumsum(tmp, dim=1)
            action_pred[:, :, :2] = tmp.to(action_pred.dtype)
        else:
            action_pred[:, :, :2] = torch.cumsum(action_pred[:, :, :2], dim=1)

        if self.learn_angle:
            action_pred[:, :, 2:] = F.normalize(action_pred[:, :, 2:].clone(), dim=-1)

        return (dist_pred, action_pred), new_states
